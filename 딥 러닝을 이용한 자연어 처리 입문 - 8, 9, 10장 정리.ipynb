{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c681f7dd",
   "metadata": {},
   "source": [
    "<h1>8장 정리</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15820e",
   "metadata": {},
   "source": [
    "1. RNN (Recurrent Neural Network) : 입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델<br>\n",
    "![RNN](https://user-images.githubusercontent.com/115343559/230359078-0485035c-d9a4-4345-ad85-b9c1233881bd.PNG)\n",
    "\n",
    "  RNN : 장기 의존성 문제 => 바닐라 RNN의 시점이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생<br>\n",
    "  * 참고한 RNN 관련 영상\n",
    "  https://www.youtube.com/watch?v=PahF2hZM6cs&t=448s\n",
    "  <br><br>\n",
    "2. LSTM : 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정한다.<br>\n",
    "  * 참고한 LSTM 관련 영상\n",
    "  https://www.youtube.com/watch?v=bX6GLbpw-A4\n",
    "  <br><br>\n",
    "\n",
    "3. GRU : LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 복잡한 LSTM의 구조를 간단화시킴<br><br>\n",
    "4. RNNLM : RNN으로 만든 언어 모델\n",
    "![RNNLM](https://user-images.githubusercontent.com/115343559/230361100-082ba7b0-ad94-4429-a488-3cd1cbedc601.PNG)\n",
    "  훈련할 때, 모델이 t 시점의 실제 알고있는 정답을 t+1 시점의 입력으로 사용하는 훈련 기법을 교사 강요(teacher forcing)라 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fe228",
   "metadata": {},
   "source": [
    "<h1>9장 정리</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8662c7",
   "metadata": {},
   "source": [
    "1. 워드 임베딩(Word Embedding) : 단어를 밀집 벡터의 형태로 표현하는 방법, 분산 표현을 이용하여 단어 간 의미적 유사성을 벡터화하는 작업<br><br>\n",
    "\n",
    "2. 워드투벡터(Word2Vec) : 벡터 차원이 단어 집합의 크기가 될 필요가 없음. 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현하여 단어 벡터 간 유의미한 유사도를 계산 가능\n",
    "- CBOW(Continuous Bag of Words) : 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법\n",
    "- Skip-Gram : 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법\n",
    "<br><br><br>\n",
    "  포워드 신경망 언어 모델(NNLM)의 느린 학습 속도와 정확도를 개선하여 탄생함\n",
    "  \n",
    "  |                | NNLM                  |Word2Vec            |\n",
    "  |----------------|-----------------------|--------------------|\n",
    "  |예측 대상       |다음 단어              |중심 단어           |\n",
    "  |참고 대상       |예측 단어 이전 단어    |예측 단어 전, 후    |\n",
    "\n",
    "  <br>\n",
    "- 네거티브 샘플링(Negative Sampling) : Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법\n",
    "![SGNS](https://user-images.githubusercontent.com/115343559/230361204-045be596-fa62-4050-a117-e9cf9349f552.PNG)\n",
    "\n",
    "  SGNS는 위의 그림과 같이 중심 단어와 주변 단어가 모두 입력이 되고, 이 두 단어가 실제로 윈도우 크기 내에 존재하는 이웃 관계인지 그 확률을 예측\n",
    "  <br><br>\n",
    "3. 케라스 임베딩 층(Keras Embedding layer) : 훈련 데이터의 단어들에 대해 워드 임베딩을 수행하는 도구 Embedding()을 제공\n",
    "![Keras](https://user-images.githubusercontent.com/115343559/230361257-3195c47d-cb99-4eb3-9ead-42585dc98e7f.PNG)\n",
    "\n",
    "  임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 함\n",
    "  <br><br>\n",
    "4. 사전 훈련된 워드 임베딩(Pre-Trained Word Embedding) : 이미 훈련되어져 있는 워드 임베딩을 가져와서 이를 임베딩 벡터로 사용하기도 함. 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용하는 것이 성능의 개선을 가져올 수 있음.\n",
    "<br><br>\n",
    "5. ELMo(Embeddings from Language Model) : 워드 임베딩 방법론으로 사전 훈련된 언어 모델을 사용\n",
    "- biLM(Bidirectional Language Model) : ELMo는 양쪽 방향의 언어 모델을 둘 다 학습하여 활용\n",
    "![ELMo](https://user-images.githubusercontent.com/115343559/230361313-8e2c1e91-5399-4746-85ec-c49ef8e80078.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645047d",
   "metadata": {},
   "source": [
    "<h1>10장 정리</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b3b87",
   "metadata": {},
   "source": [
    "1. 스팸 메일 분류하기\n",
    "<br>\n",
    "  해당 예에서는 정상 메일 샘플에 비해 스팸 메일 샘플이 매우 적음. 이렇게 레이블이 불균형한 경우에는 데이터를 나눌 때 훈련 데이터와 테스트 데이터에 각 레이블의 분포가 고르게 분포되도록 하는 것이 중요. train_test_split()에 stratify의 인자로 레이블 데이터를 넣으면 레이블의 분포가 고르게 분포됨.\n",
    "<br><br>\n",
    "2. BiLSTM으로 한국어 스팀 리뷰 감성 분류하기\n",
    "<br>\n",
    "   양방향 LSTM은 뒤의 문맥까지 고려하기 위해서 순방향 LSTM과 역방향 LSTM 셀을 함께 사용하여 출력층에서 예측 시에 두 가지 정보를 모두 사용함. 순방향 LSMT의 경우에는 마지막 시점의 은닉 상태를 반환하고, 역방향 LSTM의 경우에는 첫번째 시점의 은닉 상태를 반환함. 아래의 구조를 통해 양방향 LSTM으로 텍스트 분류를 수행할 수 있음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9837a5",
   "metadata": {},
   "source": [
    "※ 원래는 코드도 넣으려고 했으나, 코드의 전체적인 흐름을 보는게 중요하다고 판단하여 코드는 교재로 직접 보는게 좋을 것 같아 따로 넣지 않았음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
